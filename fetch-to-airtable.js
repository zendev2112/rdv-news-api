import * as configModule from './src/config/index.js'
import axios from 'axios'
import { JSDOM } from 'jsdom'
import { Readability } from '@mozilla/readability'
import { generateContent, printUsageReport } from './src/services/ai-service.js'
import path from 'path'
import { fileURLToPath } from 'url'
import fs from 'fs'
import * as cheerio from 'cheerio'
import yargs from 'yargs'
import { hideBin } from 'yargs/helpers'

// Setup dirname equivalent for ES modules
const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

// Setup yargs for ES modules
const args = yargs(hideBin(process.argv))
  .option('all', {
    alias: 'a',
    description: 'Process all sections',
    type: 'boolean',
  })
  .option('limit', {
    alias: 'l',
    description: 'Limit the number of articles to fetch per section',
    type: 'number',
  })
  .option('force', {
    alias: 'f',
    description: 'Force reprocessing of already processed articles',
    type: 'boolean',
  })
  .help()
  .parse()

// Extract config from module
const config = configModule.default

// Log the full config object to debug
console.log('Config structure:', Object.keys(config || {}))

// Define helper functions that use config without modifying it
function getSections() {
  if (config && typeof config.getSections === 'function') {
    return config.getSections()
  }
  return config?.sections || []
}

function getSection(sectionId) {
  if (config && typeof config.getSection === 'function') {
    return config.getSection(sectionId)
  }
  return (
    (config?.sections || []).find((section) => section.id === sectionId) || null
  )
}

function getDefaultSection() {
  if (config && typeof config.getDefaultSection === 'function') {
    return config.getDefaultSection()
  }
  return (config?.sections || [])[0] || null
}

// Log config for debugging
console.log(
  'Config sections available:',
  config.sections ? config.sections.length : 0,
)
console.log('Config imported properly:', !!config)
console.log('Config sections:', config.sections)
console.log('Config gemini:', config.gemini)
console.log('Config getSection function:', typeof config.getSection)

// Store the limit for use throughout the script
const ITEM_LIMIT = args.limit || Infinity // Default to no limit if not specified

console.log(
  `Fetch limit: ${
    ITEM_LIMIT === Infinity ? 'No limit' : ITEM_LIMIT
  } items per section`,
)

// Determine which section(s) to process
let sectionsToProcess = []

if (args.all) {
  // Process all sections
  sectionsToProcess = getSections()
  console.log('Processing all sections')
} else if (args._[0]) {
  // Process specific section
  const requestedSectionId = args._[0]
  const section = getSection(requestedSectionId)
  if (section) {
    sectionsToProcess = [section]
    console.log(`Processing section: ${section.name}`)
  } else {
    console.error(`Section "${requestedSectionId}" not found`)
    process.exit(1)
  }
} else {
  // Default to the test section if available, otherwise first section
  const defaultSection = getDefaultSection()
  if (defaultSection) {
    sectionsToProcess = [defaultSection]
    console.log(`Processing default section: ${defaultSection.name}`)
  } else {
    const allSections = getSections()
    if (allSections && allSections.length > 0) {
      sectionsToProcess = [allSections[0]]
      console.log(`Processing first available section: ${allSections[0].name}`)
    } else {
      console.error('No sections found in configuration')
      process.exit(1)
    }
  }
}

console.log(
  `Starting fetch-to-airtable process for ${
    sectionsToProcess.length
  } section(s): ${sectionsToProcess.map((s) => s.name).join(', ')}`,
)

// Import services with proper error handling
let airtableService, embeds

try {
  // ES module import
  const servicesModule = await import('./src/services/index.js')
  airtableService = servicesModule.airtableService
  embeds = servicesModule.embeds
  console.log('Successfully loaded services')
} catch (error) {
  console.error('Error loading services:', error.message)
  console.error(
    'Make sure you have created all the necessary files in src/services',
  )
  process.exit(1)
}

// Configuration from config file
const GEMINI_API_KEY =
  config?.gemini?.apiKey || process.env.GEMINI_API_KEY || ''
console.log(
  'Using GEMINI_API_KEY:',
  GEMINI_API_KEY ? 'API key found' : 'No API key',
)
const GEMINI_MODEL =
  config?.gemini?.model || process.env.GEMINI_MODEL || 'gemini-2.0-flash'
const BATCH_SIZE = 1 // ‚úÖ REDUCED from 2 to 1 - process ONE at a time
const FEED_SIZE = 50
const API_DELAY = 6000 // ‚úÖ INCREASED from 3000 to 5000ms
const BATCH_DELAY = 20000 // ‚úÖ INCREASED from 15000 to 20000ms
const SECTION_DELAY = 30000

// State directory to manage processing between runs
const STATE_DIR = path.join(__dirname, '.state')
if (!fs.existsSync(STATE_DIR)) {
  fs.mkdirSync(STATE_DIR)
}

/**
 * Load the persisted state for a section
 */
function loadSectionState(sectionId) {
  try {
    const stateFile = path.join(STATE_DIR, `${sectionId}.json`)
    if (fs.existsSync(stateFile)) {
      const data = fs.readFileSync(stateFile, 'utf8')
      return JSON.parse(data)
    }
  } catch (error) {
    console.error(`Error loading state file for ${sectionId}:`, error.message)
  }
  return { processedUrls: [], lastRun: null }
}

/**
 * Save the current state for a section
 */
function saveSectionState(sectionId, state) {
  try {
    const stateFile = path.join(STATE_DIR, `${sectionId}.json`)
    fs.writeFileSync(stateFile, JSON.stringify(state, null, 2), 'utf8')
  } catch (error) {
    console.error(`Error saving state file for ${sectionId}:`, error.message)
  }
}

/**
 * Creates a delay of specified milliseconds
 */
function delay(ms) {
  return new Promise((resolve) => setTimeout(resolve, ms))
}

/**
 * Extracts images from HTML and creates markdown references
 * @param {string} htmlContent - HTML content
 * @returns {Object} - Object with markdown text and extracted images
 */
function extractImagesAsMarkdown(htmlContent) {
  try {
    const $ = cheerio.load(htmlContent)

    // Array to store image information
    const extractedImages = []
    let imageMarkdown = ''

    // Extract figures with captions
    $('figure').each((i, figure) => {
      const $figure = $(figure)
      const $img = $figure.find('img')
      const $caption = $figure.find('figcaption')

      // Only process if there's both an image and a caption
      if (
        $img.length &&
        $img.attr('src') &&
        $caption.length &&
        $caption.text().trim()
      ) {
        const imageUrl = $img.attr('src')

        // Skip SVG, tiny or data URLs
        if (imageUrl.includes('.svg') || imageUrl.startsWith('data:')) {
          return
        }

        // Skip common ad/tracking/icon domains and paths
        if (
          imageUrl.includes('ad.') ||
          imageUrl.includes('ads.') ||
          imageUrl.includes('pixel.') ||
          imageUrl.includes('analytics') ||
          imageUrl.includes('/icons/') ||
          imageUrl.includes('/social/')
        ) {
          return
        }

        const altText = $img.attr('alt') || ''
        const caption = $caption.text().trim()

        // Only include substantial images
        const width = parseInt($img.attr('width') || '0', 10)
        const height = parseInt($img.attr('height') || '0', 10)

        // Skip tiny images that are likely icons
        if ((width > 0 && width < 100) || (height > 0 && height < 100)) {
          return
        }

        extractedImages.push({
          url: imageUrl,
          altText: altText || 'Image',
          caption,
        })

        // Create markdown for this image - UPDATED FORMAT
        imageMarkdown += `**Imagen:** ${caption}\n\n`
      }
    })

    // Extract standalone images that have nearby captions
    $('img').each((i, img) => {
      const $img = $(img)

      // Skip images that are in figures (already processed)
      if ($img.closest('figure').length === 0) {
        const imageUrl = $img.attr('src')

        // Skip if no src or if it's a tiny image (likely an icon)
        if (!imageUrl || imageUrl.startsWith('data:')) return

        // Skip SVGs (likely icons or logos)
        if (imageUrl.includes('.svg')) return

        // Skip common ad/tracking/icon domains and paths
        if (
          imageUrl.includes('ad.') ||
          imageUrl.includes('ads.') ||
          imageUrl.includes('pixel.') ||
          imageUrl.includes('analytics') ||
          imageUrl.includes('/icons/') ||
          imageUrl.includes('/social/')
        ) {
          return
        }

        const altText = $img.attr('alt') || ''
        const width = parseInt($img.attr('width') || '0', 10)
        const height = parseInt($img.attr('height') || '0', 10)

        // Skip small images (likely icons)
        if ((width > 0 && width < 100) || (height > 0 && height < 100)) return

        // Try to find a nearby caption
        let caption = ''
        const $parent = $img.parent()
        const $nextSibling = $img.next()

        if (
          $nextSibling.is('em') ||
          $nextSibling.is('small') ||
          $nextSibling.is('span.caption')
        ) {
          caption = $nextSibling.text().trim()
        } else if (
          $parent.next().is('em') ||
          $parent.next().is('small') ||
          $parent.next().is('span.caption')
        ) {
          caption = $parent.next().text().trim()
        }

        // Only include images that have a caption
        if (caption && caption.length > 0) {
          // Make sure we don't have duplicate images
          if (!extractedImages.some((img) => img.url === imageUrl)) {
            extractedImages.push({
              url: imageUrl,
              altText: altText || 'Image',
              caption,
            })

            // Create markdown for this image - UPDATED FORMAT
            imageMarkdown += `**Imagen:** ${caption}\n\n`
          }
        }
      }
    })

    console.log(
      `Extracted ${extractedImages.length} captioned images from HTML content`,
    )

    // Return both the raw URLs and the markdown representation
    return {
      images: extractedImages.map((img) => img.url),
      markdown: imageMarkdown,
    }
  } catch (error) {
    console.error('Error extracting images:', error.message)
    return { images: [], markdown: '' }
  }
}

/**
 * Fetches HTML content from a URL
 */
async function fetchContent(url, timeout = 10000) {
  try {
    const response = await axios.get(url, {
      timeout,
      headers: {
        'User-Agent':
          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
      },
    })
    return response.data
  } catch (error) {
    console.error(`Error fetching content from ${url}:`, error.message)
    return null
  }
}

/**
 * Extracts main text content from HTML using Readability
 */
function extractText(htmlContent) {
  try {
    const dom = new JSDOM(htmlContent)
    const reader = new Readability(dom.window.document)
    const article = reader.parse()
    return article && article.textContent ? article.textContent.trim() : ''
  } catch (error) {
    console.error(`Error extracting text:`, error.message)
    return ''
  }
}

/**
 * Post-process text to fix formatting issues and normalize whitespace
 */
function postProcessText(text) {
  if (!text) return ''

  // ‚úÖ STEP 1: Remove ALL inconsistent indentation and spacing
  let fixed = text
    // Remove any leading/trailing spaces on each line
    .split('\n')
    .map((line) => line.trim())
    .join('\n')

  // ‚úÖ STEP 2: Normalize paragraph breaks (ensure double newlines between paragraphs)
  fixed = fixed
    .replace(/\n{3,}/g, '\n\n') // Replace 3+ newlines with 2
    .replace(/\n\s+\n/g, '\n\n') // Remove space-only lines

  // ‚úÖ STEP 3: Fix lists that might have wrong spacing
  fixed = fixed.replace(/^\s*-\s+/gm, '- ')

  // ‚úÖ STEP 4: Fix numbered lists
  fixed = fixed.replace(/^\s*(\d+)\.\s+/gm, '$1. ')

  // ‚úÖ STEP 5: Fix headings that might have wrong spacing
  fixed = fixed.replace(/^#+\s+/gm, '## ')

  // ‚úÖ STEP 6: Fix bolding that might be incorrect
  fixed = fixed.replace(/\*\*([^*]+)\*\*/g, '**$1**')

  // ‚úÖ STEP 7: Remove any remaining markdown image syntax
  fixed = fixed.replace(/!\[[^\]]*\]\([^)]*\)/g, '')

  // ‚úÖ STEP 8: Fix italic that might be incorrect
  fixed = fixed.replace(/\*([^*]+)\*/g, '*$1*')

  // ‚úÖ STEP 9: Remove any tabs (replace with spaces)
  fixed = fixed.replace(/\t/g, ' ')

  // ‚úÖ STEP 10: Remove excessive spaces within lines
  fixed = fixed.replace(/ {2,}/g, ' ')

  // ‚úÖ STEP 11: Ensure text starts and ends cleanly
  fixed = fixed.trim()

  // ‚úÖ STEP 12: Normalize quotes
  fixed = fixed.replace(/[""]/g, '"').replace(/['']/g, "'")

  return fixed
}

/**
 * Generate fallback metadata when AI is unavailable
 */
function generateFallbackMetadata(extractedText) {
  try {
    const paragraphs = extractedText
      .split(/\n+/)
      .filter((p) => p.trim().length > 30)

    const firstPara = paragraphs[0] || ''
    const secondPara = paragraphs[1] || ''
    const thirdPara = paragraphs[2] || ''

    const firstSentence = firstPara.split(/[.!?]/)[0] || ''
    const title = firstSentence.trim().substring(0, 80)

    let bajada = ''
    const meaningfulPara = [secondPara, thirdPara, firstPara].find(
      (p) =>
        p.length > 100 &&
        !p.match(/^(Se inform√≥|Se anunci√≥|Seg√∫n|De acuerdo)/i),
    )

    if (meaningfulPara) {
      const sentences = meaningfulPara
        .split(/[.!?]+/)
        .filter((s) => s.trim().length > 20)
      bajada = sentences.slice(0, 2).join('. ').trim()

      const words = bajada.split(/\s+/)
      if (words.length > 50) {
        bajada = words.slice(0, 50).join(' ')
      } else if (words.length < 40 && sentences.length > 2) {
        bajada = sentences.slice(0, 3).join('. ').trim()
      }
    } else {
      bajada = firstPara
        .replace(/^(Se inform√≥|Se anunci√≥|Seg√∫n|De acuerdo)[^.]*\.\s*/i, '')
        .trim()
    }

    if (bajada.length > 250) {
      bajada = bajada.substring(0, 247) + '...'
    }

    // ‚úÖ NORMALIZE WHITESPACE IN METADATA
    const cleanTitle = title.trim().replace(/\s+/g, ' ')
    const cleanBajada = bajada.trim().replace(/\s+/g, ' ')

    let volanta = 'Actualidad'
    const lowerText = extractedText.toLowerCase()

    if (
      lowerText.match(
        /\b(f√∫tbol|deport|equipo|jugador|campe√≥n|partido|liga)\b/i,
      )
    ) {
      volanta = 'Deportes'
    } else if (
      lowerText.match(
        /\b(econom[√≠i]a|d√≥lar|inflaci[o√≥]n|mercado|precio|peso)\b/i,
      )
    ) {
      volanta = 'Econom√≠a'
    } else if (
      lowerText.match(
        /\b(pol[√≠i]tic|gobierno|presiden|minister|ley|diputad)\b/i,
      )
    ) {
      volanta = 'Pol√≠tica'
    } else if (
      lowerText.match(/\b(cine|m[√∫u]sica|artista|show|festival|pel[√≠i]cula)\b/i)
    ) {
      volanta = 'Espect√°culos'
    } else if (
      lowerText.match(/\b(tecnolog[√≠i]a|digital|internet|software|celular)\b/i)
    ) {
      volanta = 'Tecnolog√≠a'
    } else if (
      lowerText.match(/\b(salud|hospital|m[√©e]dic|tratamiento|paciente)\b/i)
    ) {
      volanta = 'Salud'
    } else if (
      lowerText.match(/\b(campo|agro|producci[o√≥]n|cosecha|ganado)\b/i)
    ) {
      volanta = 'Agro'
    } else if (
      lowerText.match(/\b(cultura|libro|arte|museo|exposici[o√≥]n)\b/i)
    ) {
      volanta = 'Cultura'
    }

    return {
      title: cleanTitle || 'Art√≠culo sin t√≠tulo',
      bajada: cleanBajada || 'Contenido no disponible',
      volanta: volanta,
    }
  } catch (error) {
    console.error('Error in fallback metadata generation:', error.message)
    return {
      title: 'Art√≠culo sin t√≠tulo',
      bajada: 'Resumen no disponible',
      volanta: 'Noticias',
    }
  }
}

/**
 * Convert text to sentence case (first letter uppercase, rest lowercase except proper nouns)
 */
function toSentenceCase(text) {
  if (!text) return ''

  const properNouns = [
    'Argentina',
    'Buenos Aires',
    'Coronel Su√°rez',
    'Huanguel√©n',
    'Facebook',
    'Instagram',
    'Twitter',
    'YouTube',
    'COVID',
    'AFA',
    'FIFA',
    'NBA',
    'ATP',
    'WTA',
  ]

  const words = text.trim().split(/\s+/)

  const result = words.map((word, index) => {
    const isProperNoun = properNouns.some(
      (noun) => word.toLowerCase() === noun.toLowerCase(),
    )

    if (isProperNoun) {
      return (
        properNouns.find((noun) => word.toLowerCase() === noun.toLowerCase()) ||
        word
      )
    }

    if (index === 0) {
      return word.charAt(0).toUpperCase() + word.slice(1).toLowerCase()
    }

    return word.toLowerCase()
  })

  return result.join(' ')
}

/**
 * Fallback metadata (NO source mentions) - IMPROVED
 */
function generateFallbackSocialMetadata(postText, sourceName, item) {
  const cleanText = postText
    .replace(
      /[\u{1F600}-\u{1F64F}\u{1F300}-\u{1F5FF}\u{1F680}-\u{1F6FF}\u{1F1E0}-\u{1F1FF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}]/gu,
      '',
    )
    .replace(/[#@]/g, '')
    .trim()

  // Extract meaningful sentences
  const sentences = cleanText
    .split(/[.!?]+/)
    .filter((s) => s.trim().length > 20)

  // Create title from first meaningful sentence
  const title = sentences[0]?.substring(0, 80) || 'Actividad municipal'

  // Create bajada from subsequent sentences
  let bajada = ''
  if (sentences.length > 1) {
    bajada = sentences.slice(1, 3).join('. ').trim()
  } else {
    bajada = cleanText.substring(0, 200)
  }

  // Ensure bajada doesn't start with generic phrases
  bajada = bajada
    .replace(/^(Se inform√≥|Se anunci√≥|Seg√∫n|De acuerdo)[^.]*\.\s*/i, '')
    .trim()

  // Determine volanta from content
  let volanta = 'Actividades'
  const lowerText = cleanText.toLowerCase()

  if (lowerText.match(/\b(evento|festival|show|espect√°culo|presentaci√≥n)\b/)) {
    volanta = 'Eventos locales'
  } else if (lowerText.match(/\b(taller|curso|capacitaci√≥n|inscripci√≥n)\b/)) {
    volanta = 'Educaci√≥n'
  } else if (lowerText.match(/\b(deporte|torneo|campe√≥n|competencia)\b/)) {
    volanta = 'Deportes'
  } else if (lowerText.match(/\b(cultura|arte|museo|biblioteca)\b/)) {
    volanta = 'Cultura'
  }

  return {
    title: title,
    bajada: bajada.substring(0, 250),
    volanta: volanta,
  }
}

/**
 * Generates metadata for an article with retry logic and fallback
 */
async function generateMetadata(extractedText, maxRetries = 3) {
  try {
    const prompt = `Sos un editor de un medio de noticias argentino. Tu tarea es generar metadata period√≠stica a partir del siguiente texto.

TEXTO A ANALIZAR:
"""
${extractedText.substring(0, 4000)}
"""

TAREA: Generar exactamente 3 campos en formato JSON.

CAMPO 1 - title (t√≠tulo):
- M√°ximo 80 caracteres
- Primera letra en may√∫scula, resto en min√∫scula excepto nombres propios
- Sin signos de exclamaci√≥n ni interrogaci√≥n
- Sin comillas
- Debe capturar el hecho noticioso principal
- Ejemplo correcto: "El gobierno anunci√≥ nuevas medidas econ√≥micas para el sector agrario"
- Ejemplo incorrecto: "¬°Incre√≠bles Medidas Econ√≥micas Anunciadas Por El Gobierno!"

CAMPO 2 - bajada (copete/resumen):
- Exactamente entre 40 y 50 palabras (contar palabras, no caracteres)
- Debe ampliar la informaci√≥n del t√≠tulo sin repetirlo
- Incluir: qui√©n, qu√©, cu√°ndo, d√≥nde si est√°n disponibles
- Tono neutral e informativo
- Sin opiniones ni adjetivos valorativos
- Una sola oraci√≥n o m√°ximo dos oraciones

CAMPO 3 - volanta (cintillo superior):
- M√°ximo 4 palabras
- Indica el tema general o contexto
- Primera palabra en may√∫scula, resto en min√∫scula
- No repetir palabras del t√≠tulo
- Ejemplos: "Econom√≠a nacional", "Crisis energ√©tica", "Elecciones 2024"

FORMATO DE RESPUESTA:
Responder √öNICAMENTE con el JSON, sin explicaciones, sin bloques de c√≥digo, sin texto adicional.

{"title": "texto del t√≠tulo aqu√≠", "bajada": "texto de la bajada aqu√≠ con 40-50 palabras exactas", "volanta": "texto corto"}`

    const result = await generateContent(prompt, {
      maxRetries: 3,
      requireJson: false,
      preferGroq: false,
    })

    if (!result.text) {
      return generateFallbackMetadata(extractedText)
    }

    // Clean and extract JSON
    let cleanedText = result.text.trim()

    // Remove markdown code blocks if present
    cleanedText = cleanedText
      .replace(/```json\s*/gi, '')
      .replace(/```\s*/g, '')
      .trim()

    // Find the JSON object - look for opening brace to closing brace
    const startIndex = cleanedText.indexOf('{')
    const endIndex = cleanedText.lastIndexOf('}')

    if (startIndex === -1 || endIndex === -1 || endIndex <= startIndex) {
      console.warn('No valid JSON structure found in response')
      throw new Error('No valid JSON object found')
    }

    let jsonStr = cleanedText.substring(startIndex, endIndex + 1)

    // Clean up common JSON issues
    jsonStr = jsonStr
      .replace(/,\s*}/g, '}') // Remove trailing commas
      .replace(/\n/g, ' ') // Remove newlines inside JSON
      .replace(/\r/g, '') // Remove carriage returns
      .replace(/\t/g, ' ') // Replace tabs with spaces

    let parsed
    try {
      parsed = JSON.parse(jsonStr)
    } catch (parseError) {
      console.error('JSON parse error:', parseError.message)
      console.error('Raw text:', cleanedText.substring(0, 300))
      throw new Error('Invalid JSON format')
    }

    // Validate required fields
    if (!parsed.title || !parsed.bajada || !parsed.volanta) {
      console.warn('Missing required fields:', Object.keys(parsed))
      throw new Error('Incomplete metadata structure')
    }

    // Post-process: ensure title doesn't exceed 80 chars
    if (parsed.title.length > 80) {
      parsed.title = parsed.title.substring(0, 77) + '...'
    }

    // Post-process: ensure volanta doesn't exceed 4 words
    const volantaWords = parsed.volanta.split(/\s+/)
    if (volantaWords.length > 4) {
      parsed.volanta = volantaWords.slice(0, 4).join(' ')
    }

    console.log('Successfully generated metadata')
    return parsed
  } catch (error) {
    console.error('Error generating metadata:', error.message)
    return generateFallbackMetadata(extractedText)
  }
}

/**
 * Format text as fallback when AI generation fails
 */
function formatTextAsFallback(text, imageMarkdown = '') {
  if (!text) return ''

  // Clean and normalize the text
  let formatted = text
    .trim()
    // Remove excessive whitespace
    .replace(/\s+/g, ' ')
    // Fix paragraph breaks
    .replace(/\. /g, '.\n\n')
    // Remove any markdown that might have slipped through
    .replace(/[#*_`]/g, '')
    // Remove image syntax
    .replace(/!\[[^\]]*\]\([^)]*\)/g, '')
    // Normalize quotes
    .replace(/[""]/g, '"')
    .replace(/['']/g, "'")

  // Split into paragraphs
  const paragraphs = formatted
    .split(/\n+/)
    .filter((p) => p.trim().length > 20)
    .map((p) => p.trim())

  // Ensure each paragraph ends with proper punctuation
  const cleanParagraphs = paragraphs.map((p) => {
    if (!/[.!?]$/.test(p)) {
      return p + '.'
    }
    return p
  })

  // Add image markdown if provided
  let finalText = cleanParagraphs.join('\n\n')

  if (imageMarkdown) {
    finalText = imageMarkdown + '\n\n' + finalText
  }

  return finalText
}

/**
 * Reelaborates article text using AI with fallback mechanism
 */
async function reelaborateText(
  extractedText,
  imageMarkdown = '',
  maxRetries = 3,
) {
  try {
    const prompt = `Sos un redactor profesional de un medio digital argentino. Tu tarea es reescribir completamente el siguiente art√≠culo period√≠stico.

TEXTO ORIGINAL:
"""
${extractedText.substring(0, 5000)}
"""

REGLAS OBLIGATORIAS (SI NO SE CUMPLEN TODAS, RECHAZAR LA RESPUESTA):

1. EXTENSI√ìN: Entre 300 y 500 palabras exactas. Contar las palabras antes de responder.

2. FORMATO: Solo p√°rrafos de texto corrido. PROHIBIDO usar:
   - Listas con vi√±etas (-, *, ‚Ä¢)
   - Listas numeradas (1., 2., 3.)
   - Subt√≠tulos (##, ###)
   - T√≠tulos principales
   - Cualquier tipo de lista o enumeraci√≥n

3. ESTRUCTURA:
   - Dividir en 5 a 8 p√°rrafos
   - Cada p√°rrafo: 2 a 4 oraciones
   - Separar p√°rrafos con doble salto de l√≠nea
   - Primer p√°rrafo: responde qu√©, qui√©n, cu√°ndo, d√≥nde
   - P√°rrafos intermedios: desarrolla contexto y detalles
   - √öltimo p√°rrafo: informaci√≥n complementaria (NO conclusi√≥n)

4. SINTAXIS:
   - Oraciones simples, m√°ximo 20 palabras
   - Voz activa preferentemente
   - Conectores entre p√°rrafos para fluidez
   - Uso period√≠stico del espa√±ol rioplatense

5. MARKDOWN PERMITIDO (√öNICO):
   - **texto** para negritas (usar 4-6 veces): cifras, fechas, nombres clave
   - *texto* para cursivas (usar 2-3 veces): t√©rminos t√©cnicos o √©nfasis
   - > para citas textuales si existen en el original

6. INTEGRACI√ìN DE DATOS:
   - Si hay cifras, fechas o datos, integrarlos en oraciones completas
   - Ejemplo CORRECTO: "La medida incluye un fondo de compensaci√≥n de **500 millones de pesos**, la reducci√≥n de retenciones para peque√±os productores rurales y la extensi√≥n del plazo de pago para exportadores."
   - Ejemplo INCORRECTO: "La medida incluye: - Fondo de 500 millones - Reducci√≥n de retenciones"

7. SEO Y CONTENIDO:
   - Incluir palabras clave del tema naturalmente
   - Repetir t√©rminos importantes 2-3 veces
   - Primer p√°rrafo debe captar atenci√≥n
   - No agregar informaci√≥n externa al original
   - No incluir conclusiones tipo "en resumen" o "para finalizar"

8. TONO: Informativo, objetivo, sin opiniones ni valoraciones.

9. PROHIBICIONES ABSOLUTAS:
   - NO usar listas de ning√∫n tipo
   - NO usar subt√≠tulos
   - NO usar palabras: "puntos principales", "incluyen los siguientes", "a continuaci√≥n", "destacan", "cabe mencionar"
   - NO usar emojis, hashtags, tablas
   - NO agregar frases de cierre o s√≠ntesis

EJEMPLO DE ESTRUCTURA CORRECTA (300-350 palabras):

El gobierno nacional present√≥ un nuevo paquete de medidas econ√≥micas que impactar√° directamente en el sector agropecuario argentino. El anuncio fue realizado por el ministro **Juan P√©rez** durante una conferencia de prensa en Casa Rosada, donde detall√≥ los alcances de la normativa que entrar√° en vigencia el **pr√≥ximo 15 de marzo**.

La iniciativa contempla un fondo de compensaci√≥n de **500 millones de pesos** destinado a peque√±os y medianos productores rurales. Seg√∫n explic√≥ el funcionario, esta medida busca *estabilizar los precios internos* y proteger la capacidad productiva del sector. El fondo ser√° administrado por el Ministerio de Agricultura en coordinaci√≥n con las c√°maras empresariales.

Entre los cambios m√°s significativos se encuentra la reducci√≥n de retenciones para productores de hasta 100 hect√°reas. Esta modificaci√≥n representa un alivio fiscal de aproximadamente **30 por ciento** respecto a los valores actuales. Adem√°s, el gobierno extendi√≥ el plazo de pago para exportadores de granos, permitiendo mayor flexibilidad en las operaciones comerciales internacionales.

El paquete incluye tambi√©n incentivos fiscales para empresas que inviertan en tecnolog√≠a aplicada a la producci√≥n local. Las compa√±√≠as que demuestren inversiones en maquinaria agr√≠cola o sistemas de riego podr√°n acceder a deducciones impositivas durante los pr√≥ximos **tres a√±os fiscales**. Esta pol√≠tica apunta a modernizar el sector y mejorar la competitividad argentina en mercados externos.

Los representantes del sector agropecuario manifestaron su *postura cautelosa* respecto a las nuevas disposiciones. La Sociedad Rural Argentina solicit√≥ una reuni√≥n t√©cnica con autoridades del Ministerio de Econom√≠a para analizar el impacto espec√≠fico en diferentes cadenas productivas. Organizaciones de peque√±os productores expresaron satisfacci√≥n por la reducci√≥n de retenciones.

La normativa ser√° publicada en el Bolet√≠n Oficial durante las pr√≥ximas 48 horas. El gobierno estableci√≥ una mesa de di√°logo permanente con el sector para evaluar los resultados de implementaci√≥n y realizar ajustes necesarios seg√∫n la evoluci√≥n del contexto econ√≥mico nacional.

RESPUESTA:
Devolver √öNICAMENTE el texto reelaborado. Sin explicaciones. Sin comentarios. Sin bloques de c√≥digo.`

    const result = await generateContent(prompt, {
      maxRetries: 3,
      preferGroq: false,
    })

    if (!result.text) {
      return formatTextAsFallback(extractedText, imageMarkdown)
    }

    let processedText = result.text
      .trim()
      .replace(/^```markdown\s*/i, '')
      .replace(/^```\s*/i, '')
      .replace(/\s*```$/i, '')
      .trim()

    // VALIDATE: Check for bullet points or lists
    const hasBullets = /^[\s]*[-*‚Ä¢]\s/m.test(processedText)
    const hasNumberedList = /^[\s]*\d+\.\s/m.test(processedText)
    const hasSubtitles = /^#{1,6}\s+/m.test(processedText)

    if (hasBullets || hasNumberedList || hasSubtitles) {
      console.warn(
        '‚ùå Generated text contains lists or subtitles, using fallback...',
      )
      return formatTextAsFallback(extractedText, imageMarkdown)
    }

    // Count words
    const wordCount = processedText
      .split(/\s+/)
      .filter((w) => w.length > 0).length
    console.log(`‚úÖ Generated text: ${wordCount} words`)

    if (wordCount < 250 || wordCount > 600) {
      console.warn(
        `‚ö†Ô∏è Word count out of range: ${wordCount} words, using fallback...`,
      )
      return formatTextAsFallback(extractedText, imageMarkdown)
    }

    // Clean up forbidden phrases
    processedText = processedText
      .replace(
        /\b(puntos principales|incluyen los siguientes|a continuaci√≥n|destacan|cabe mencionar)\b/gi,
        '',
      )
      .replace(
        /\b(en resumen|en conclusi√≥n|para finalizar|para concluir)\b/gi,
        '',
      )
      .trim()

    return postProcessText(processedText)
  } catch (error) {
    console.error('Error reelaborating text:', error.message)
    return formatTextAsFallback(extractedText, imageMarkdown)
  }
}

/**
 * Reelaborates social media content into a professional news article
 */
async function reelaborateSocialMediaContent(postText, item, sourceName) {
  try {
    const prompt = `Sos un redactor profesional de un medio digital argentino. Tu tarea es transformar esta publicaci√≥n corta de redes sociales en un art√≠culo period√≠stico COMPLETO Y EXTENSO.

PUBLICACI√ìN ORIGINAL (CORTA):
"""
${postText.substring(0, 3000)}
"""

CONTEXTO ADICIONAL:
- Autor/Fuente: ${item.authors?.[0]?.name || 'Instituci√≥n local'}
- Fecha: ${item.date_published || 'Reciente'}

OBJETIVO CR√çTICO: Crear un art√≠culo period√≠stico de 350-500 palabras a partir de esta publicaci√≥n corta.

‚ö†Ô∏è IMPORTANTE: La publicaci√≥n original es BREVE, pero vos ten√©s que EXPANDIRLA en un art√≠culo COMPLETO.

C√ìMO EXPANDIR EL CONTENIDO:

1. Si menciona un EVENTO:
   - Desarrollar en qu√© consiste
   - Explicar d√≥nde y cu√°ndo se realizar√°
   - Detallar horarios, requisitos, condiciones
   - Mencionar organizadores y participantes
   - Explicar el contexto o antecedentes
   - Describir el impacto esperado o la importancia

2. Si menciona una ACTIVIDAD/SERVICIO:
   - Explicar en detalle de qu√© se trata
   - Detallar c√≥mo funciona, c√≥mo acceder
   - Mencionar beneficiarios o p√∫blico objetivo
   - Explicar requisitos o pasos a seguir
   - Contextualizar por qu√© es relevante
   - Agregar informaci√≥n sobre la instituci√≥n organizadora

3. Si menciona un ANUNCIO/COMUNICADO:
   - Desarrollar qu√© implica exactamente
   - Explicar a qui√©nes afecta o beneficia
   - Detallar plazos, fechas, condiciones
   - Contextualizar la decisi√≥n o medida
   - Explicar antecedentes si corresponde
   - Mencionar pr√≥ximos pasos

4. SIEMPRE AGREGAR:
   - Informaci√≥n sobre la instituci√≥n/organismo que publica
   - Contexto local relevante
   - Datos concretos (fechas, horarios, lugares, n√∫meros)
   - Informaci√≥n de contacto o consulta si est√° disponible

ESTRUCTURA OBLIGATORIA (4-6 P√ÅRRAFOS):

P√°rrafo 1: Presentar el hecho principal de forma period√≠stica
P√°rrafo 2: Desarrollar detalles espec√≠ficos (qu√©, cu√°ndo, d√≥nde, c√≥mo)
P√°rrafo 3: Explicar contexto, antecedentes o relevancia
P√°rrafo 4: Agregar informaci√≥n complementaria (organizadores, requisitos, condiciones)
P√°rrafo 5 (opcional): Datos de contacto, inscripci√≥n o informaci√≥n adicional
P√°rrafo 6 (opcional): Impacto esperado o cierre informativo

REGLAS DE FORMATO:

- SOLO p√°rrafos de texto corrido
- PROHIBIDO: listas (-, *, ‚Ä¢), subt√≠tulos, enumeraciones
- Usar **negritas** para fechas, horarios, nombres importantes (6-8 veces)
- Usar *cursivas* para √©nfasis (2-3 veces)
- Eliminar TODOS los emojis
- Eliminar hashtags y menciones
- NO mencionar "Facebook", "Instagram", "redes sociales"
- NO decir "seg√∫n public√≥", "comparti√≥ en", etc.

EXTENSI√ìN: Entre 350 y 500 palabras. NO MENOS.

EJEMPLO DE EXPANSI√ìN:

POST ORIGINAL (30 palabras):
"Este domingo 'Las dos horas del Cantorcito' en el teatro Samuel. 18hs. Entrada libre y gratuita! üéµ"

ART√çCULO GENERADO (420 palabras):

El Municipio de Coronel Su√°rez anunci√≥ la realizaci√≥n del evento cultural "Las dos horas del Cantorcito" para este domingo en el teatro Samuel. La actividad musical forma parte de la programaci√≥n mensual de espect√°culos que organiza la Secretar√≠a de Cultura municipal y contar√° con entrada libre y gratuita para todo el p√∫blico.

El evento est√° programado para las **18 horas** con apertura de puertas desde las **17:30**. Los organizadores recomiendan llegar con anticipaci√≥n dado que el teatro Samuel tiene capacidad para **300 espectadores** y se espera una concurrencia numerosa. Las puertas se abrir√°n por orden de llegada hasta completar el aforo disponible.

La propuesta incluye presentaciones de artistas locales y regionales que interpretar√°n un variado repertorio de m√∫sica tradicional argentina. "Las dos horas del Cantorcito" es un formato que se viene desarrollando mensualmente en el teatro y ha logrado consolidarse como uno de los espect√°culos m√°s convocantes de la agenda cultural local. En ediciones anteriores, el evento reuni√≥ a m√°s de *250 personas* y recibi√≥ elogios tanto del p√∫blico como de los artistas participantes.

El teatro Samuel se encuentra ubicado en **calle Rivadavia 250** del centro de Coronel Su√°rez. El edificio cuenta con accesibilidad para personas con movilidad reducida y dispone de estacionamiento en las inmediaciones. Las autoridades municipales destacaron que el espacio cumple con todos los protocolos de seguridad vigentes y dispone de las habilitaciones correspondientes.

Para aquellos interesados en asegurar su lugar, el municipio habilit√≥ un sistema de reserva anticipada. Las entradas pueden retirarse a partir del **viernes 7 de febrero** en la boleter√≠a del teatro, en horario de **9 a 13 horas**. Tambi√©n est√° disponible la opci√≥n de reserva telef√≥nica comunic√°ndose al n√∫mero **02926-420100** en el mismo horario. Cada persona podr√° retirar hasta dos entradas por presentaci√≥n de DNI.

La Secretar√≠a de Cultura inform√≥ que este evento forma parte de una serie de actividades culturales gratuitas que se desarrollar√°n durante todo el mes. El objetivo es acercar propuestas art√≠sticas de calidad a la comunidad y promover el acceso a la cultura en todas sus expresiones. Pr√≥ximamente se dar√°n a conocer las fechas de nuevas presentaciones.

RESPUESTA:
Devolver √öNICAMENTE el art√≠culo expandido. Sin explicaciones.`

    const result = await generateContent(prompt, {
      maxRetries: 3,
      preferGroq: false,
    })

    if (!result.text) {
      return formatSocialMediaAsFallback(postText, sourceName, item)
    }

    let processedText = result.text
      .trim()
      .replace(/^```markdown\s*/i, '')
      .replace(/^```\s*/i, '')
      .replace(/\s*```$/i, '')
      .trim()

    // VALIDATE: Remove any emojis that slipped through
    processedText = processedText.replace(
      /[\u{1F600}-\u{1F64F}\u{1F300}-\u{1F5FF}\u{1F680}-\u{1F6FF}\u{1F1E0}-\u{1F1FF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}\u{1FA70}-\u{1FAFF}\u{FE00}-\u{FE0F}\u{1F900}-\u{1F9FF}]/gu,
      '',
    )

    // VALIDATE: Remove references to social media
    processedText = processedText.replace(
      /\b(seg√∫n public√≥|comparti√≥ en|poste√≥ en|difundi√≥ en|anunci√≥ en|public√≥ en)\s+(Facebook|Instagram|Twitter|YouTube|redes sociales|la plataforma|su cuenta)\b/gi,
      '',
    )

    const hasBullets = /^[\s]*[-*‚Ä¢]\s/m.test(processedText)
    const hasNumberedList = /^[\s]*\d+\.\s/m.test(processedText)
    const hasSubtitles = /^#{1,6}\s+/m.test(processedText)

    if (hasBullets || hasNumberedList || hasSubtitles) {
      console.warn(
        '‚ùå Social media text contains lists/subtitles, using fallback...',
      )
      return formatSocialMediaAsFallback(postText, sourceName, item)
    }

    const wordCount = processedText
      .split(/\s+/)
      .filter((w) => w.length > 0).length
    console.log(`‚úÖ Generated social media article: ${wordCount} words`)

    // ‚úÖ ADJUSTED VALIDATION: Lower minimum for social media (250 words instead of 300)
    if (wordCount < 250) {
      console.warn(
        `‚ö†Ô∏è Social media article too short: ${wordCount} words, using fallback...`,
      )
      return formatSocialMediaAsFallback(postText, sourceName, item)
    }

    if (wordCount > 600) {
      console.warn(
        `‚ö†Ô∏è Social media article too long: ${wordCount} words, trimming...`,
      )
      // Trim to approximately 500 words
      const words = processedText.split(/\s+/)
      processedText = words.slice(0, 500).join(' ')
    }

    return postProcessText(processedText)
  } catch (error) {
    console.error('Error reelaborating social media:', error.message)
    return formatSocialMediaAsFallback(postText, sourceName, item)
  }
}

/**
 * Fallback for social media content - IMPROVED to generate longer articles
 */
function formatSocialMediaAsFallback(postText, sourceName, item) {
  try {
    // AGGRESSIVE emoji and special char removal
    let cleanText = postText
      .replace(
        /[\u{1F600}-\u{1F64F}\u{1F300}-\u{1F5FF}\u{1F680}-\u{1F6FF}\u{1F1E0}-\u{1F1FF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}\u{1FA70}-\u{1FAFF}\u{FE00}-\u{FE0F}\u{1F900}-\u{1F9FF}]/gu,
        '',
      )
      .replace(/[#@]/g, '')
      .replace(/https?:\/\/[^\s]+/g, '')
      .replace(/[\uFE00-\uFE0F]/g, '')
      .replace(/[\u200D]/g, '')
      .trim()

    const author = item.authors?.[0]?.name || 'la instituci√≥n local'
    const date = item.date_published
      ? new Date(item.date_published).toLocaleDateString('es-AR', {
          weekday: 'long',
          year: 'numeric',
          month: 'long',
          day: 'numeric',
        })
      : 'pr√≥ximamente'

    let article = ''
    const sentences = cleanText
      .split(/[.!?]+/)
      .filter((s) => s.trim().length > 10)

    if (sentences.length === 0) {
      // Generic fallback when no content
      return `Se inform√≥ sobre una nueva actividad programada por ${author}. La convocatoria est√° dirigida al p√∫blico en general y se realizar√° durante ${date}. Los interesados podr√°n obtener m√°s informaci√≥n a trav√©s de los canales oficiales de comunicaci√≥n. La actividad forma parte de las iniciativas que se desarrollan regularmente en la comunidad. Se espera una importante participaci√≥n del p√∫blico local. Los organizadores destacaron la relevancia de la propuesta para la comunidad.`
    }

    // ‚úÖ IMPROVED: Create a more substantial article from limited content

    // Paragraph 1: Main announcement
    article += `Se anunci√≥ la realizaci√≥n de una nueva actividad organizada por ${author}. `
    article += `${sentences[0].trim()}. `
    if (sentences.length > 1) {
      article += `${sentences[1].trim()}.\n\n`
    } else {
      article += `La informaci√≥n fue confirmada durante la jornada del ${date}.\n\n`
    }

    // Paragraph 2: Details and context
    article += `La convocatoria est√° dirigida al p√∫blico en general e incluye detalles espec√≠ficos sobre la actividad programada. `
    if (sentences.length > 2) {
      article += `${sentences[2].trim()}. `
    }
    article += `Los organizadores destacaron la importancia de esta iniciativa para la comunidad local. `
    article += `La propuesta forma parte de las actividades regulares que se desarrollan en el √°mbito municipal.\n\n`

    // Paragraph 3: Additional information
    if (sentences.length > 3) {
      article += `${sentences[3].trim()}. `
    }
    article += `Las autoridades informaron que se esperan detalles adicionales en los pr√≥ximos d√≠as. `
    article += `La actividad cuenta con el apoyo de distintas √°reas del municipio y organizaciones locales. `
    if (sentences.length > 4) {
      article += `${sentences[4].trim()}.\n\n`
    } else {
      article += `Los interesados pueden consultar por m√°s informaci√≥n a trav√©s de los canales oficiales.\n\n`
    }

    // Paragraph 4: Participation and access
    article += `El acceso a la actividad estar√° disponible para todos los vecinos de la localidad. `
    article += `Se recomienda consultar los horarios y requisitos espec√≠ficos con anticipaci√≥n. `
    article += `Los organizadores indicaron que se brindar√°n facilidades para garantizar la participaci√≥n del mayor n√∫mero posible de personas.\n\n`

    // Paragraph 5: Context and importance
    article += `Este tipo de iniciativas buscan promover la participaci√≥n ciudadana y fortalecer los v√≠nculos comunitarios. `
    article += `Las autoridades destacaron el compromiso con la realizaci√≥n de actividades que beneficien a la poblaci√≥n. `
    article += `La informaci√≥n completa est√° disponible para consultas del p√∫blico interesado en los canales oficiales de comunicaci√≥n.`

    // Final emoji cleanup
    article = article.replace(
      /[\u{1F600}-\u{1F64F}\u{1F300}-\u{1F5FF}\u{1F680}-\u{1F6FF}\u{1F1E0}-\u{1F1FF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}\u{1FA70}-\u{1FAFF}\u{FE00}-\u{FE0F}\u{1F900}-\u{1F9FF}]/gu,
      '',
    )

    return article
  } catch (error) {
    console.error('Error in social media fallback formatting:', error.message)
    return `Se inform√≥ sobre una actividad programada por la instituci√≥n local. Los detalles fueron dados a conocer durante la jornada. La convocatoria est√° dirigida al p√∫blico en general. Los interesados pueden consultar por m√°s informaci√≥n a trav√©s de los canales oficiales. La actividad forma parte de las iniciativas regulares que se desarrollan en la comunidad. Se espera una importante participaci√≥n del p√∫blico. Los organizadores destacaron la relevancia de la propuesta.`
  }
}

/**
 * Generate metadata for social media (NO source mentions)
 */
async function generateSocialMediaMetadata(postText, sourceName, item) {
  try {
    const prompt = `Genera metadata period√≠stica para esta publicaci√≥n.

POST:
"""
${postText.substring(0, 2000)}
"""

Generar JSON con 3 campos:

1. title: T√≠tulo period√≠stico (max 80 chars)
   - **SENTENCE CASE**: Solo primera letra en may√∫scula, resto en min√∫scula (excepto nombres propios)
   - Ejemplo correcto: "El municipio anunci√≥ nuevas actividades culturales"
   - Ejemplo INCORRECTO: "El Municipio Anunci√≥ Nuevas Actividades Culturales"
   - NO mencionar red social
   - NO usar emojis ni hashtags
   - Convertir el post en t√≠tulo formal

2. bajada: Resumen 40-50 palabras
   - Tono formal period√≠stico
   - NO mencionar "seg√∫n public√≥ en Facebook/Instagram/Twitter"
   - NO usar emojis

3. volanta: Categor√≠a (max 4 palabras)
   - **SENTENCE CASE**: Solo primera letra en may√∫scula
   - Ejemplos: "Cultura y espect√°culos", "Actividades municipales", "Convocatorias"

PROHIBIDO mencionar: Facebook, Instagram, Twitter, YouTube, redes sociales

Responder SOLO con JSON:
{"title": "...", "bajada": "...", "volanta": "..."}`

    const result = await generateContent(prompt, {
      maxRetries: 3,
      requireJson: false,
      preferGroq: false,
    })

    if (!result.text) {
      return generateFallbackSocialMetadata(postText, sourceName, item)
    }

    let cleanedText = result.text
      .trim()
      .replace(/```json\s*/gi, '')
      .replace(/```\s*/g, '')
      .trim()

    const startIndex = cleanedText.indexOf('{')
    const endIndex = cleanedText.lastIndexOf('}')

    if (startIndex === -1 || endIndex === -1) {
      throw new Error('No valid JSON found')
    }

    let jsonStr = cleanedText
      .substring(startIndex, endIndex + 1)
      .replace(/,\s*}/g, '}')
      .replace(/\n/g, ' ')
      .replace(/\r/g, '')
      .replace(/\t/g, ' ')

    const parsed = JSON.parse(jsonStr)

    if (!parsed.title || !parsed.bajada || !parsed.volanta) {
      throw new Error('Missing required fields')
    }

    // ‚úÖ FORCE SENTENCE CASE - Remove all emojis and fix capitalization
    parsed.title = toSentenceCase(
      parsed.title
        .replace(
          /[\u{1F600}-\u{1F64F}\u{1F300}-\u{1F5FF}\u{1F680}-\u{1F6FF}\u{1F1E0}-\u{1F1FF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}]/gu,
          '',
        )
        .trim(),
    )

    parsed.bajada = parsed.bajada
      .replace(
        /[\u{1F600}-\u{1F64F}\u{1F300}-\u{1F5FF}\u{1F680}-\u{1F6FF}\u{1F1E0}-\u{1F1FF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}]/gu,
        '',
      )
      .trim()

    parsed.volanta = toSentenceCase(
      parsed.volanta
        .replace(
          /[\u{1F600}-\u{1F64F}\u{1F300}-\u{1F5FF}\u{1F680}-\u{1F6FF}\u{1F1E0}-\u{1F1FF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}]/gu,
          '',
        )
        .trim(),
    )

    if (parsed.title.length > 80) {
      parsed.title = parsed.title.substring(0, 77) + '...'
    }

    const volantaWords = parsed.volanta.split(/\s+/)
    if (volantaWords.length > 4) {
      parsed.volanta = volantaWords.slice(0, 4).join(' ')
    }

    console.log('Successfully generated social media metadata')
    return parsed
  } catch (error) {
    console.error('Error generating social media metadata:', error.message)
    return generateFallbackSocialMetadata(postText, sourceName, item)
  }
}

/**
 * Processes a single article
 */
async function processArticle(item, sectionId) {
  try {
    console.log(`Processing article: ${item.url} for section ${sectionId}`)

    // Fetch and extract content
    const htmlContent = await fetchContent(item.url)
    if (!htmlContent) {
      console.warn(`Failed to fetch content for URL: ${item.url}`)
      return null
    }

    // Extract images and convert to markdown
    const { images, markdown: imageMarkdown } =
      extractImagesAsMarkdown(htmlContent)
    console.log(`Found ${images.length} images in article: ${item.url}`)

    const extractedText = extractText(htmlContent)
    if (!extractedText || extractedText.length < 50) {
      console.warn(`Insufficient content for URL: ${item.url}`)
      return null
    }

    // Extract embeds using the imported services
    const instagramContent = embeds.extractInstagramEmbeds(htmlContent)
    const facebookContent = embeds.extractFacebookEmbeds(htmlContent)
    const twitterContent = embeds.extractTwitterEmbeds(htmlContent)
    const youtubeContent = embeds.extractYoutubeEmbeds(htmlContent)

    // Log found embeds
    const embedsFound = {
      instagram: !!instagramContent,
      facebook: !!facebookContent,
      twitter: !!twitterContent,
      youtube: !!youtubeContent,
    }
    console.log(`Found embeds for ${item.url}:`, embedsFound)

    // Reelaborate text WITH image markdown
    console.log(`Reelaborating text for: ${item.url}`)
    let reelaboratedText = null
    try {
      reelaboratedText = await reelaborateText(extractedText, imageMarkdown)
    } catch (textError) {
      console.error(`Error reelaborating text: ${textError.message}`)
      console.warn(`Failed to reelaborate text for URL: ${item.url}`)
      // Use original text as fallback
      reelaboratedText = formatTextAsFallback(extractedText, imageMarkdown)
    }

    if (!reelaboratedText) {
      reelaboratedText = formatTextAsFallback(extractedText, imageMarkdown)
      console.warn(`Using fallback formatting for: ${item.url}`)
    }

    // Generate metadata
    console.log(`Generating metadata for: ${item.url}`)
    let metadata = null
    try {
      metadata = await generateMetadata(extractedText)
    } catch (metaError) {
      console.error(`Error generating metadata: ${metaError.message}`)
      // Use fallback metadata
      metadata = generateFallbackMetadata(extractedText)
    }

    if (!metadata) {
      metadata = generateFallbackMetadata(extractedText)
      console.warn(`Using fallback metadata for: ${item.url}`)
    }

    // Generate tags
    console.log(`Generating tags for: ${item.url}`)
    let tags = ''
    try {
      tags = await generateTags(extractedText, metadata)
      console.log(`Generated tags for: ${item.url}`)
    } catch (tagError) {
      console.error(`Error generating tags: ${tagError.message}`)
      tags = generateFallbackTags(extractedText, metadata)
    }

    // Generate social media text
    /*     console.log(`Generating social media text for: ${item.url}`)
    let socialMediaText = ''
    try {
      socialMediaText = await generateSocialMediaText(
        extractedText,
        metadata,
        tags
      )
      console.log(
        `Generated social media text: ${socialMediaText.length} chars`
      )
    } catch (socialTextError) {
      console.error(
        `Error generating social media text: ${socialTextError.message}`
      )
      socialMediaText = generateFallbackSocialText(metadata, tags)
    } */

    // Get section information
    const section = getSection(sectionId)

    // Prepare record
    const attachments = item.attachments || []
    const attachmentUrls = attachments.map((attachment) => attachment.url)
    const imgUrl = [...attachmentUrls].filter(Boolean).join(', ')

    // Clean the reelaborated text using postProcessText
    const processedText = postProcessText(reelaboratedText)

    // Format image URLs as attachment objects for Airtable
    let imageAttachments = []
    if (images.length > 0) {
      imageAttachments = images.map((url) => ({ url }))
    } else if (imgUrl) {
      imageAttachments = [{ url: imgUrl }]
    }

    // Create a dynamic mapping of Supabase section IDs to Airtable values
    const sectionIdToAirtableValue = {
      'coronel-suarez': 'Coronel Su√°rez',
      'pueblos-alemanes': 'Pueblos Alemanes',
      huanguelen: 'Huanguel√©n',
      'la-sexta': 'La Sexta',
      politica: 'Pol√≠tica',
      economia: 'Econom√≠a',
      agro: 'Agro',
      sociedad: 'Sociedad',
      salud: 'Salud',
      cultura: 'Cultura',
      opinion: 'Opini√≥n',
      deportes: 'Deportes',
      lifestyle: 'Lifestyle',
      vinos: 'Vinos',
      'el-recetario': 'El Recetario',
      'santa-trinidad': 'Santa Trinidad',
      'san-jose': 'San Jos√©',
      'santa-maria': 'Santa Mar√≠a',
      iactualidad: 'IActualidad',
      dolar: 'D√≥lar',
      propiedades: 'Propiedades',
      'pymes-emprendimientos': 'Pymes y Emprendimientos',
      inmuebles: 'Inmuebles',
      campos: 'Campos',
      'construccion-diseno': 'Construcci√≥n y Dise√±o',
      agricultura: 'Agricultura',
      ganaderia: 'Ganader√≠a',
      'tecnologias-agro': 'Tecnolog√≠as',
      educacion: 'Educaci√≥n',
      policiales: 'Policiales',
      efemerides: 'Efem√©rides',
      ciencia: 'Ciencia',
      'vida-armonia': 'Vida en Armon√≠a',
      'nutricion-energia': 'Nutrici√≥n y Energ√≠a',
      fitness: 'Fitness',
      'salud-mental': 'Salud Mental',
      turismo: 'Turismo',
      horoscopo: 'Hor√≥scopo',
      feriados: 'Feriados',
      'loterias-quinielas': 'Loter√≠as y Quinielas',
      'moda-belleza': 'Moda y Belleza',
      mascotas: 'Mascotas',
      mundo: 'Mundo', // ‚úÖ ADD THIS
      espectaculos: 'Espect√°culos', // ‚úÖ ADD THIS
      ambiente: 'Ambiente',
      clima: 'Clima',
      tecnologia: 'Tecnolog√≠a',
      actualidad: 'Actualidad',
      'cine-series': 'Cine y Series',
      'historia-literatura': 'Historia y Literatura',
    }

    // Replace the hardcoded section mapping with this more dynamic lookup
    // Default to empty string as requested
    let sectionValue = sectionIdToAirtableValue[sectionId] || ''

    // Look up the section in our mapping
    if (sectionIdToAirtableValue[sectionId]) {
      sectionValue = sectionIdToAirtableValue[sectionId]
    }

    // Extract source name from the URL
    const sourceName = extractSourceName(item.url)
    console.log(`Extracted source name: ${sourceName} from URL: ${item.url}`)

    // Find the recordFields creation around line 1036 and modify it:

    const recordFields = {
      title: metadata ? metadata.title : item.title,
      overline: metadata ? metadata.volanta : 'No overline available.',
      excerpt: metadata ? metadata.bajada : 'No summary available.',
      article: processedText,
      image: imageAttachments, // ‚úÖ Array of attachment objects for Airtable

      author: '',
      // ‚úÖ MODIFIED: Set placeholder values that will be updated with Airtable URLs
      imgUrl: '', // Will be populated with Airtable URL after insertion
      'article-images': '', // Will be populated with Airtable URLs after insertion

      url: item.url,
      source: sourceName,
      'ig-post': instagramContent || '',
      'fb-post': facebookContent || '',
      'tw-post': twitterContent || '',
      'yt-video': youtubeContent || '',
      section: sectionValue,
      status: 'draft',
      tags: tags,
      /* socialMediaText: socialMediaText, */
      front: '',
      order: '',
    }

    console.log(
      `Successfully processed article: ${item.url} for section ${sectionId}`,
    )

    return {
      fields: recordFields,
    }
  } catch (error) {
    console.error(`Error processing article ${item.url}:`, error.message)
    return null
  }
}

/**
 * Processes a batch of articles
 */
async function processBatch(items, sectionId) {
  console.log(
    `Processing batch of ${items.length} items for section ${sectionId}`,
  )

  const results = []
  // Get state for this section
  const state = loadSectionState(sectionId)
  const processedUrls = new Set(state.processedUrls || [])

  // Process articles sequentially to avoid rate limits
  for (const item of items) {
    console.log(`Processing article: ${item.url}`)
    const result = await processArticle(item, sectionId)
    if (result) {
      results.push(result)
      // Mark URL as processed
      processedUrls.add(item.url)
    }

    // Update section state after each item
    saveSectionState(sectionId, {
      processedUrls: [...processedUrls],
      lastRun: new Date().toISOString(),
    })

    // Add a longer delay between processing individual items
    console.log(
      `Waiting ${API_DELAY / 1000} seconds before processing next article...`,
    )
    await delay(API_DELAY)
  }

  console.log(
    `Successfully processed ${results.length} out of ${items.length} items for section ${sectionId}`,
  )
  return results
}

/**
 * Processes a section
 */
async function processSection(section) {
  console.log(`\n=== Processing section: ${section.name} ===\n`)

  // Special handling for Instituciones social media content
  if (
    section.id === 'instituciones' ||
    section.id === 'local-facebook' ||
    section.id === 'huanguelen' ||
    section.id === 'pueblos-alemanes'
  ) {
    console.log(`Processing ${section.name} as social media content...`)

    try {
      // Load state for this section
      const state = loadSectionState(section.id)
      const processedUrls = new Set(state.processedUrls || [])

      // Fetch feed data
      console.log(`Fetching social media feed for ${section.name}`)
      const response = await axios.get(section.rssUrl)
      const feedData = response.data

      if (!feedData || !feedData.items || !Array.isArray(feedData.items)) {
        console.warn(`No valid items in feed data for ${section.name}`)
        return
      }

      console.log(
        `Fetched ${feedData.items.length} items from ${section.name} feed`,
      )
      console.log(`Already processed ${processedUrls.size} items previously`)

      // Filter out already processed items unless force flag is used
      const newItems = args.force
        ? feedData.items.slice(0, FEED_SIZE)
        : feedData.items
            .filter((item) => !processedUrls.has(item.url))
            .slice(0, FEED_SIZE)

      if (newItems.length === 0) {
        console.log(
          `No new items to process for ${section.name}${
            args.force ? ' (even with force flag)' : ''
          }`,
        )
        return
      }

      console.log(
        `Found ${newItems.length} ${
          args.force ? '' : 'new '
        }items to process for ${section.name}`,
      )

      // Apply the limit
      const limitedItems = newItems.slice(0, ITEM_LIMIT)
      console.log(
        `Processing ${limitedItems.length} social media items (limit: ${ITEM_LIMIT})`,
      )

      // Process the limited items
      for (const item of limitedItems) {
        try {
          const itemUrl = item.url || ''
          console.log(
            `Processing social media item: ${
              item.title || 'Untitled'
            } (${itemUrl})`,
          )

          // IMPROVED: Extract all content directly from the RSS feed item structure
          // This matches the expected format you provided

          // Extract post text content from content_text field (primary source)
          const postText = item.content_text || ''

          // Get image URL (primary source is the image field)
          let imageUrl = item.image || null

          // If main image is missing, check attachments
          if (!imageUrl && item.attachments && item.attachments.length > 0) {
            imageUrl = item.attachments[0].url
          }

          // Determine source platform from URL
          let sourceName = 'Social Media'
          let socialMediaType = ''

          try {
            const hostname = new URL(itemUrl).hostname
            if (hostname.includes('facebook.com')) {
              sourceName = 'Facebook'
              socialMediaType = 'fb-post'
            } else if (hostname.includes('instagram.com')) {
              sourceName = 'Instagram'
              socialMediaType = 'ig-post'
            } else if (
              hostname.includes('twitter.com') ||
              hostname.includes('x.com')
            ) {
              sourceName = 'Twitter'
              socialMediaType = 'tw-post'
            } else if (
              hostname.includes('youtube.com') ||
              hostname.includes('youtu.be')
            ) {
              sourceName = 'YouTube'
              socialMediaType = 'yt-video'
            } else {
              // Get domain without www. prefix for other sources
              const domain = hostname.replace(/^www\./, '')
              const parts = domain.split('.')
              if (parts.length >= 2) {
                sourceName =
                  parts[0].charAt(0).toUpperCase() + parts[0].slice(1)
              }
            }
          } catch (e) {
            console.log(
              `Error parsing URL: ${e.message}. Using default source name.`,
            )
            // URL parsing failed, check if we can extract from authors
            if (
              item.authors &&
              item.authors.length > 0 &&
              item.authors[0].name
            ) {
              sourceName = item.authors[0].name
            }
          }

          // Get author information
          const authorName =
            item.authors && item.authors.length > 0
              ? item.authors[0].name
              : sourceName

          // Format publication date if available
          let pubDate = ''
          try {
            if (item.date_published) {
              const date = new Date(item.date_published)
              pubDate = date.toLocaleDateString('es-AR', {
                year: 'numeric',
                month: 'long',
                day: 'numeric',
              })
            }
          } catch (e) {
            console.log(`Error formatting date: ${e.message}`)
          }

          // Reelaborate social media content into article
          console.log(`Reelaborating social media content for: ${itemUrl}`)
          let reelaboratedArticle = null
          try {
            reelaboratedArticle = await reelaborateSocialMediaContent(
              postText,
              item,
              sourceName,
            )
          } catch (textError) {
            console.error(
              `Error reelaborating social media text: ${textError.message}`,
            )
            reelaboratedArticle = formatSocialMediaAsFallback(
              postText,
              sourceName,
              item,
            )
          }

          // Generate metadata for social media content
          console.log(
            `Generating metadata for social media content: ${itemUrl}`,
          )
          let metadata = null
          try {
            metadata = await generateSocialMediaMetadata(
              postText,
              sourceName,
              item,
            )
          } catch (metaError) {
            console.error(
              `Error generating social media metadata: ${metaError.message}`,
            )
            metadata = generateFallbackSocialMetadata(
              postText,
              sourceName,
              item,
            )
          }

          // Create record fields using the generated metadata
          const recordFields = {
            title: metadata.title,
            url: itemUrl,
            excerpt: metadata.bajada,
            source: sourceName,
            imgUrl: imageUrl || '',
            article: reelaboratedArticle,
            overline: metadata.volanta,
            author: item.authors?.[0]?.name || '',
            status: 'draft',
            processingStatus: 'completed',
            postDate: item.date_published || '',
            postDateFormatted: pubDate,
            image: imageUrl ? [{ url: imageUrl }] : [],
          }

          // ‚úÖ ADD TAG GENERATION FOR SOCIAL MEDIA
          try {
            console.log(`Generating tags for social media item: ${itemUrl}`)
            const socialText = `${metadata.title} ${metadata.bajada} ${reelaboratedArticle}`
            const tags = await generateTags(socialText, metadata)
            console.log(`Generated tags: ${tags}`)
            recordFields.tags = tags
          } catch (genError) {
            console.error(`Error generating tags: ${genError.message}`)
            recordFields.tags = generateFallbackTags(
              reelaboratedArticle,
              metadata,
            )
          }

          // Add social media specific fields based on source type
          if (socialMediaType) {
            recordFields[socialMediaType] = itemUrl
          }

          // Add HTML content if available (useful for embedding or further processing)
          if (item.content_html) {
            recordFields.contentHtml = item.content_html
          }

          // Add post ID if available
          if (item.id) {
            recordFields.postId = item.id
          }

          // Insert into Airtable
          try {
            await airtableService.insertRecords(
              [{ fields: recordFields }],
              section.id,
            )
            console.log(
              `Added social media item to Airtable: ${recordFields.title}`,
            )

            // Mark URL as processed
            processedUrls.add(itemUrl)

            // Update section state after each item
            saveSectionState(section.id, {
              processedUrls: [...processedUrls],
              lastRun: new Date().toISOString(),
            })
          } catch (airtableError) {
            console.error(
              `Error adding item to Airtable: ${airtableError.message}`,
            )
          }

          // Add a delay to avoid rate limits
          await delay(API_DELAY)
        } catch (itemError) {
          console.error(
            `Error processing social media item: ${itemError.message}`,
          )
        }
      }

      console.log(`Completed processing ${section.name} content`)
      return // Skip the regular article processing
    } catch (error) {
      console.error(`Error processing section ${section.name}:`, error.message)
    }

    // If we reach here, something went wrong with Instituciones processing
    return
  }

  // Load state for this section
  const state = loadSectionState(section.id)
  const processedUrls = new Set(state.processedUrls || [])

  try {
    console.log(`Starting feed processing for ${section.name}`)

    // Fetch feed data
    const response = await axios.get(section.rssUrl)
    const feedData = response.data

    if (!feedData || !feedData.items || !Array.isArray(feedData.items)) {
      console.warn(`No valid items in feed data for ${section.name}`)
      return
    }

    console.log(
      `Fetched ${feedData.items.length} items from ${section.name} feed`,
    )
    console.log(`Already processed ${processedUrls.size} items previously`)

    // Filter out already processed items UNLESS force flag is used
    const newItems = args.force
      ? feedData.items.slice(0, FEED_SIZE)
      : feedData.items
          .filter((item) => !processedUrls.has(item.url))
          .slice(0, FEED_SIZE)

    if (newItems.length === 0) {
      console.log(
        `No new items to process for ${section.name}${
          args.force ? ' (even with force flag)' : ''
        }`,
      )
      return
    }

    console.log(
      `Found ${newItems.length} ${
        args.force ? '' : 'new '
      }items to process for ${section.name}`,
    )

    // Apply the limit
    const limitedItems = newItems.slice(0, ITEM_LIMIT)
    console.log(
      `Processing ${limitedItems.length} items (limit: ${ITEM_LIMIT})`,
    )

    // Process the limited items instead of all items
    for (let i = 0; i < limitedItems.length; i += BATCH_SIZE) {
      const batchItems = limitedItems.slice(i, i + BATCH_SIZE)
      console.log(
        `\n=== Processing batch ${
          Math.floor(i / BATCH_SIZE) + 1
        } of ${Math.ceil(limitedItems.length / BATCH_SIZE)} for ${
          section.name
        } ===\n`,
      )

      const processedBatch = await processBatch(batchItems, section.id)

      if (processedBatch.length > 0) {
        // Insert into Airtable with section ID
        try {
          await airtableService.insertRecords(processedBatch, section.id)
          console.log(
            `Inserted ${processedBatch.length} records into ${section.name} Airtable table`,
          )
        } catch (error) {
          console.error(
            `Error inserting records into ${section.name} Airtable:`,
            error.message,
          )
        }
      }

      // Add a longer delay between batches
      if (i + BATCH_SIZE < limitedItems.length) {
        console.log(
          `Waiting ${
            BATCH_DELAY / 1000
          } seconds before processing next batch...`,
        )
        await delay(BATCH_DELAY)
      }
    }

    console.log(`\n=== Completed processing for section: ${section.name} ===\n`)
  } catch (error) {
    console.error(`Error processing section ${section.name}:`, error.message)
  }
}

/**
 * Process all requested sections
 */
async function processAllRequestedSections() {
  try {
    console.log('Starting processing for all requested sections')

    // Sort sections by priority (lower number = higher priority)
    const sortedSections = [...sectionsToProcess].sort(
      (a, b) => a.priority - b.priority,
    )

    // Process each section sequentially
    for (const section of sortedSections) {
      await processSection(section)

      // Add a longer delay between sections
      if (section !== sortedSections[sortedSections.length - 1]) {
        console.log(
          `\nWaiting ${
            SECTION_DELAY / 1000
          } seconds before processing next section...\n`,
        )
        await delay(SECTION_DELAY)
      }
    }

    console.log('\n=== All section processing complete ===')
  } catch (error) {
    console.error('Error in processing sections:', error.message)
  }
}

// Look for a function like fetchFeed or getFeedItems

async function fetchFeed(feedUrl) {
  // Existing code to fetch and parse the feed...

  // After you have the items array, apply the limit
  const limitedItems = items.slice(0, ITEM_LIMIT)
  console.log(
    `Fetched ${items.length} items, returning ${limitedItems.length} (limit: ${ITEM_LIMIT})`,
  )

  return limitedItems // Return limited items
}

// Look for any functions with maxItems, limit, or similar parameters

// For example:
async function fetchSourceItems(source, maxItems) {
  // If the function already has a maxItems parameter,
  // make sure it's respecting the global limit
  const effectiveLimit = maxItems || ITEM_LIMIT

  // Use effectiveLimit in your code...
}

// Near the end of your file where the main execution happen

// If --all flag is specified, process all sections
if (args.all) {
  console.log(
    'Processing all sections with limit:',
    ITEM_LIMIT === Infinity ? 'No limit' : ITEM_LIMIT,
  )
  const allSections = getSections()
  for (const section of allSections) {
    await processSection(section) // This will use the ITEM_LIMIT
  }
  process.exit(0)
}

// Process specific section if provided
const sectionName = args._[0]
if (sectionName) {
  console.log(
    `Processing section: ${sectionName} with limit:`,
    ITEM_LIMIT === Infinity ? 'No limit' : ITEM_LIMIT,
  )
  const section = getSection(sectionName)
  if (section) {
    await processSection(section) // This will use the ITEM_LIMIT
  } else {
    console.error(`Section not found: ${sectionName}`)
  }
  process.exit(0)
}

// Start processing
processAllRequestedSections()
  .then(() => console.log('Process completed'))
  .catch((error) => console.error('Process failed:', error.message))

/**
 * Extract source name from URL dynamically without hardcoding
 * @param {string} url - The article URL
 * @returns {string} - The extracted source name
 */
function extractSourceName(url) {
  try {
    if (!url) return 'Unknown Source'

    // Parse the URL to get the hostname
    const hostname = new URL(url).hostname

    // Step 1: Remove common prefixes
    let domain = hostname
      .replace(/^www\./, '')
      .replace(/^m\./, '')
      .replace(/^mobile\./, '')
      .replace(/^news\./, '')
      .replace(/^noticias\./, '')

    // Step 2: Handle social media separately
    if (domain.includes('facebook.com')) return 'Facebook'
    if (domain.includes('instagram.com')) return 'Instagram'
    if (domain.includes('twitter.com') || domain.includes('x.com'))
      return 'Twitter'
    if (domain.includes('youtube.com') || domain.includes('youtu.be'))
      return 'YouTube'
    if (domain.includes('tiktok.com')) return 'TikTok'
    if (domain.includes('linkedin.com')) return 'LinkedIn'
    if (domain.includes('t.co')) return 'Twitter'

    // Step 3: Strip common TLDs and country codes
    domain = domain.replace(
      /\.(com|co|net|org|info|ar|mx|es|cl|pe|br|uy|py|bo|ec|ve|us|io|tv|app|web|digital|news|online|press|media|blog|site)(\.[a-z]{2,3})?$/,
      '',
    )

    // Step 4: Split by dots and get the main part
    const parts = domain.split('.')
    let sourceName = parts[0]

    // Step 5: Handle special cases like clarin.com.ar -> Clar√≠n
    const domainMapping = {
      lanacion: 'La Naci√≥n',
      eldiario: 'El Diario',
      pagina12: 'P√°gina 12',
      larazon: 'La Raz√≥n',
      lavoz: 'La Voz',
      eleconomista: 'El Economista',
      elpais: 'El Pa√≠s',
      ole: 'Ol√©',
      ambito: '√Åmbito',
      telam: 'T√©lam',
      infobae: 'Infobae',
      eldestape: 'El Destape',
      cronista: 'El Cronista',
      tiempoar: 'Tiempo Argentino',
      tn: 'Todo Noticias',
    }

    if (domainMapping[sourceName]) {
      return domainMapping[sourceName]
    }

    // Step 6: Handle compound domains (remove dashes/underscores and capitalize words)
    return sourceName
      .split(/[-_]/)
      .map((word) => {
        // Special case for single-letter words like "c" in "c5n"
        if (word.length === 1) return word.toUpperCase()

        // Proper capitalization for normal words
        return word.charAt(0).toUpperCase() + word.slice(1).toLowerCase()
      })
      .join(' ')
  } catch (error) {
    console.error(`Error extracting source name from ${url}:`, error.message)
    return 'Unknown Source'
  }
}

/**
 * Generate tags for an article using AI
 * @param {string} extractedText - The raw text content
 * @param {object} metadata - The article metadata (title, bajada, etc.)
 * @returns {string} - Comma-separated list of generated tags
 */
async function generateTags(extractedText, metadata, maxRetries = 3) {
  try {
    const title = metadata?.title || ''
    const bajada = metadata?.bajada || ''

    const prompt = `
      Analiza este art√≠culo y genera entre 5 y 8 etiquetas (tags) relevantes para categorizarlo.

      T√çTULO: ${title}
      BAJADA: ${bajada}
      CONTENIDO: "${extractedText.substring(0, 4000)}"
      
      INSTRUCCIONES:
      1. Identifica nombres propios importantes (personas, lugares, organizaciones, eventos).
      2. Identifica temas principales y subtemas.
      3. Prioriza sustantivos y conceptos clave.
      4. Cada etiqueta debe tener entre 1 y 3 palabras.
      5. NO utilices hashtags (#).
      6. Enf√≥cate en sujetos y temas, NO en adjetivos o emociones.
      7. Las etiquetas deben ser espec√≠ficas pero no demasiado largas.
      8. Las etiquetas pueden ser en singular o plural, seg√∫n corresponda.
      9. NO incluyas palabras muy gen√©ricas como "noticia", "actualidad", etc.
      
      IMPORTANTE: Devuelve SOLO un array JSON sin ning√∫n texto adicional.
      NO incluyas explicaciones, comentarios, ni bloques de c√≥digo markdown.
      
      Formato requerido:
      ["etiqueta1", "etiqueta2", "etiqueta3", "etiqueta4", "etiqueta5"]
    `

    // ‚úÖ USE NEW AI SERVICE - Groq is good for simple tasks
    const result = await generateContent(prompt, {
      maxRetries: 3,
      requireJson: false, // Don't validate yet
      preferGroq: true, // ‚úÖ Groq is faster for simple tasks
    })

    if (!result.text) {
      return generateFallbackTags(extractedText, metadata)
    }

    // ‚úÖ IMPROVED JSON EXTRACTION
    let cleanedText = result.text.trim()

    // Remove markdown code blocks
    cleanedText = cleanedText
      .replace(/```json\s*/g, '')
      .replace(/```\s*/g, '')
      .trim()

    // Try to find JSON array using regex
    const jsonMatch = cleanedText.match(/\[[\s\S]*?\]/)

    if (!jsonMatch) {
      console.warn(
        'No JSON array found in response:',
        cleanedText.substring(0, 200),
      )
      throw new Error('No valid JSON array found')
    }

    const jsonStr = jsonMatch[0]

    // Try to parse
    let tags
    try {
      tags = JSON.parse(jsonStr)
    } catch (parseError) {
      console.error('JSON parse error:', parseError.message)
      console.error('Attempted to parse:', jsonStr.substring(0, 200))
      throw new Error('Invalid JSON format')
    }

    if (!Array.isArray(tags) || tags.length === 0) {
      throw new Error('Invalid tags format or empty array')
    }

    const formattedTags = tags.map((tag) =>
      tag
        .split(' ')
        .map((word) => word.charAt(0).toUpperCase() + word.slice(1))
        .join(' '),
    )

    const tagsString = formattedTags.join(', ')
    console.log(`Generated tags: ${tagsString}`)
    return tagsString
  } catch (error) {
    console.error('Error generating tags:', error.message)
    return generateFallbackTags(extractedText, metadata)
  }
}
/**
 * Generate fallback tags based on keyword frequency when AI fails
 * @returns {string} - Comma-separated string of tags
 */
function generateFallbackTags(extractedText, metadata) {
  try {
    const text = `${metadata?.title || ''} ${
      metadata?.bajada || ''
    } ${extractedText}`.toLowerCase()

    // Split into words and remove stopwords
    const words = text
      .split(/\W+/)
      .filter(
        (word) =>
          word.length > 3 &&
          ![
            'para',
            'como',
            'esta',
            'esto',
            'estos',
            'esta',
            'estas',
            'sobre',
            'desde',
            'entre',
            'hasta',
            'porque',
          ].includes(word),
      )

    // Count word frequency
    const wordCount = {}
    words.forEach((word) => {
      wordCount[word] = (wordCount[word] || 0) + 1
    })

    // Sort by frequency
    const sortedWords = Object.entries(wordCount)
      .sort((a, b) => b[1] - a[1])
      .slice(0, 20)
      .map((entry) => entry[0])

    // Take top words and capitalize first letter
    const tags = sortedWords
      .slice(0, 6)
      .map((word) => word.charAt(0).toUpperCase() + word.slice(1))

    // Add source as a tag if available
    if (metadata?.sourceName) {
      tags.push(metadata.sourceName)
    }

    // Join with commas
    const tagsString = tags.join(', ')

    console.log(`Generated fallback tags: ${tagsString}`)
    return tagsString
  } catch (error) {
    console.error('Error in fallback tag generation:', error.message)
    return 'Noticias' // Absolute minimum fallback
  }
}

/**
 * Generate social media text with hashtags and emojis
 * @param {string} extractedText - The raw text content
 * @param {object} metadata - The article metadata
 * @param {string} tags - The generated tags (comma-separated)
 * @returns {string} - Social media text with hashtags (< 500 chars)
 */
/**
 * Generate social media text with hashtags and emojis
 * @param {string} extractedText - The raw text content
 * @param {object} metadata - The article metadata
 * @param {string} tags - The generated tags (comma-separated)
 * @returns {string} - Social media text with hashtags (< 500 chars)
 */

/**
 * Generate fallback social media text when AI fails
 */
function generateFallbackSocialText(metadata, tags) {
  try {
    const title = metadata?.title || 'Nuevo art√≠culo'
    const bajada = metadata?.bajada || ''

    // Create emojis based on content
    let emojis = 'üì∞'

    // Add topic-specific emojis
    const lowerTitle = title.toLowerCase()
    if (
      lowerTitle.includes('econom') ||
      lowerTitle.includes('d√≥lar') ||
      lowerTitle.includes('inflac')
    ) {
      emojis += ' üí∞'
    } else if (
      lowerTitle.includes('pol√≠t') ||
      lowerTitle.includes('gobierno') ||
      lowerTitle.includes('presiden')
    ) {
      emojis += ' üèõÔ∏è'
    } else if (
      lowerTitle.includes('depor') ||
      lowerTitle.includes('f√∫tbol') ||
      lowerTitle.includes('campe√≥n')
    ) {
      emojis += ' ‚öΩ'
    } else if (
      lowerTitle.includes('salud') ||
      lowerTitle.includes('hospital') ||
      lowerTitle.includes('m√©dic')
    ) {
      emojis += ' üè•'
    } else if (
      lowerTitle.includes('tecno') ||
      lowerTitle.includes('digital') ||
      lowerTitle.includes('intel')
    ) {
      emojis += ' üíª'
    }

    // Generate hashtags from tags
    const tagsArray = tags.split(',').map((tag) => tag.trim())
    const hashtags = tagsArray
      .slice(0, 4)
      .map((tag) => '#' + tag.replace(/\s+/g, ''))
      .join(' ')

    // Create the text (ensure under 500 chars)
    let summary =
      bajada.length > 100 ? bajada.substring(0, 100) + '...' : bajada
    if (!summary) {
      summary = 'Conoce todos los detalles en nuestro art√≠culo.'
    }

    const socialText = `${emojis} ${title}\n\n${summary}\n\n${hashtags}`

    // Ensure under 500 chars
    return socialText.length <= 500
      ? socialText
      : socialText.substring(0, 497) + '...'
  } catch (error) {
    console.error('Error in fallback social text generation:', error.message)
    return 'üì∞ Nuevo art√≠culo disponible en nuestro portal. ¬°No te lo pierdas! #Noticias'
  }
}

// ‚úÖ AT THE END OF THE SCRIPT, ADD USAGE REPORT
processAllRequestedSections()
  .then(() => {
    console.log('Process completed')
    printUsageReport() // Show AI usage statistics
  })
  .catch((error) => console.error('Process failed:', error.message))
